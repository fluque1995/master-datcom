---
title: "Clustering"
subtitle: "Minería de datos: aprendizaje no supervisado y detección de anomalías"
author: "Francisco Luque Sánchez"
date: "21/12/2019"
titlepage: true
titlepage-background: "background.pdf"
headrule-color: "435488"
urlcolor: 'blue'
script-font-size: \scriptsize
nncode-block-font-size: \scriptsize
output:
    pdf_document:
        number_sections: yes
        template: eisvogel
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE)
set.seed(0)
library(ggplot2)
library(ggfortify)
library(stats)
library(fpc)
library(cluster)
library(dplyr)
library(factoextra)
library(fastDummies)
library(knitr)
```

# Introducción

En este trabajo se van a estudiar técnicas de agrupamiento de ejemplos
de una base de datos desde el punto de vista del aprendizaje no
supervisado. En el paradigma de aprendizaje no supervisado no se
dispone de un conjunto de etiquetas o categorías en las que agrupar
los datos y que son conocidas a priori, si no que se tratan de formar
grupos de elementos (_clusters_) en función de la similaridad que hay
entre ellos. Esta similaridad está usualmente definida en función de la
distancia que existe entre ellos dentro del espacio de
características. Cuanto menor sea la distancia entre dos elementos de
nuestro conjunto, más probable es que exista alguna relación entre los
mismos, y por tanto será deseable que el algoritmo de agrupamiento los
coloque dentro del mismo grupo. Por el contrario, cuanto más alejados
estén los elementos en el espacio de características, más diferencia
existirá entre ellos, y por tanto nos interesará que se emplacen en
_clusters_ diferentes.

Por tanto, para tener bien definido un algoritmo de agrupamiento,
tendremos que definir dos cosas. Por un lado, habrá que establecer una
medida de distancia o similaridad entre cualesquiera dos elementos de
nuestro conjunto de datos. Esta distancia dependerá del tipo de datos
con los que estemos trabajando. Si los datos con los que estamos
trabajando están compuestos por atributos numéricos continuos, la
distancia a utilizar será usualmente la distancia euclídea, o una
distancia de Minkowski (la distancia euclídea es un caso particular de
una distancia de Minkowski, cuando k=2). En cambio, si la información
está codificada como un conjunto de variables binarias o nominales, es
posible que estas distancias no estén bien definidas, o no tengan
sentido, y habrá que recurrir a otras funciones de distancia. Otros
tipos de datos, como textos, imágenes o series temporales, necesitarán
también de medidas de distancia propias que nos permitan comparar dos
elementos. En el desarrollo de esta práctica se estudiará cómo el uso
de distintas medidas de distancia pueden dar lugar a resultados
distintos. Por otro lado, habrá de definir qué estrategia se utiliza
para agrupar los puntos una vez que conocemos la distancia que existe
entre ellos. En esta práctica se estudiarán distintas políticas de
agrupamiento, concretamente las siguientes:

- k-means
- DBSCAN
- Clustering jerárquico
- k-medioides
- k-means difuso

Para cada algoritmo de los dictados anteriormente, se hará una breve
descripción teórica del mismo, y se harán pruebas para observar los
resultados que obtiene el mismo sobre un determinado dataset. Además,
para algunos de los algoritmos, estudiaremos cómo el cambio en la
distancia utilizada produce distintos resultados.

## Conjunto de datos empleado - Bankloan

El conjunto de datos sobre el que trabajaremos tiene el nombre de
Bankloan dataset. Es uno de los conjuntos de datos de ejemplo que se
distribuyen con el software IBM SPSS. El objetivo de dicho conjunto de
datos es una tarea de clasificación, en la que se pide predecir si un
conjunto de potenciales clientes incurrirá o no en impago. El conjunto
está compuesto por 700 registros de entrenamiento, para los cuales
tenemos registrado si incurrieron o no en impago, y 150 registros de
test, para los cuales tenemos que predecir dicha etiqueta. En este
trabajo, descartaremos los últimos 150 registros a priori, ya que nos
interesa saber el valor de impago para los individuos a la hora de
calcular algunas de las medidas que nos indicarán la calidad del
modelo que hemos calculado.

En cuanto a las variables que se registran, se tienen datos de la edad
del individuo, su nivel educativo, los años de empleo que lleva en su
actual empresa, información de deudas e ingresos, y tres columnas que
indican la probabilidad de que el individuo haya incurrido en impago
(no se nos indica cómo se han calculado dichas probabilidades). En
total, el conjunto está compuesto por 11 atributos más la clase. De
estos atributos, dos de ellos son nominales (la clase, que toma dos
valores distintos, y el nivel educativo, que toma cinco valores) y el
resto son variables numéricas. Tomaremos la columna que indica el
nivel educativo del individuo como una característica nominal porque
no conocemos si los valores que toma esta variable corresponden a una
escala ordenada, o son simplemente un código. El considerarlos una
escala ordinal cuando realmente no lo son puede introducir un sesgo en
la distancia que se calcula entre los elementos en esta variable,
comportamiento que queremos evitar.

Como hemos dicho anteriormente, aunque el conjunto de datos está
pensado originalmente para ser utilizado en un problema de
clasificación, nosotros lo utilizaremos para un problema de
agrupamiento. Utilizaremos las etiquetas para comprobar si los
algoritmos de agrupamiento son capaces de separar correctamente
los datos en las dos clases.

Comenzamos estudiando el algoritmo de _clustering_ k-medias

# K-medias

El algoritmo de clustering k-medias (también conocido como _k-means_)
es un algoritmo que trata de particionar el espacio de características
en $k$ conjuntos distintos, tratando de minimizar la distancia media
intra-cluster (dicha cantidad es la media de las distancia de cada
punto del cluster a su centro). Formalmente, el objetivo es agrupar
$n$ observaciones en $k$ conjuntos $S = \{S_1, ..., S_n\}$ tal que se
minimice la siguiente suma:

\[
    \sum_{i=1}^{k} \sum_{x \in S_i} d(x,\mu_i)^2
\]

Donde $\mu_i$ es el centro del clúster, y $d(v, w)^2$ es la distancia
al cuadrado del vector $v$ al vector $w$. Debido a que nos encontramos
ante un problema de optimización de complejidad _NP-duro_, no suele
darse la solución óptima al problema. En su lugar, se utilizan
heurísticas de cálculo que dan soluciones aproximadas. La más conocida
es el algoritmo de Lloyd. Este algoritmo calcula la solución al
problema de las k-medias de forma iterativa, hasta que se llega a un
punto estable. El algoritmo se divide en dos fases, una de asignación
de puntos a los clusters y otra de actualización de los centros. El
pseudocódigo del algoritmo es el siguiente:

1. Se generan aleatoriamente los $k$ centros en el espacio de
características. Existen varias técnicas para inicializar los
puntos.
2. Mientras no se haya llegado a un punto estable:
   - Asignación: Cada punto del conjunto de datos es asignado
   al cluster cuyo centro le es más cercano
   - Actualización: Para cada cluster, se recalcula el centro
   del mismo a partir de la media de los puntos que pertenecen
   a él
3. El algoritmo termina cuando en una iteración todos los puntos son
asignados al cluster al que pertenecían en la iteración anterior.

## Aplicación del algoritmo

Una vez hemos dado una descripción teórica del algoritmo, pasamos a
estudiar cómo aplicarlo. En primer lugar, aplicaremos el algoritmo
utilizando sólo las variables numéricas de las que
disponemos. Comenzamos cargando el conjunto de datos y seleccionando
las variables numéricas que nos interesan.

```{r}
dataset.path <- "dataset/bankloan-spss.csv"
dataset <- read.csv(dataset.path, sep = ";", dec=",")
dataset <- dataset[1:700,]

numeric.vars <- c(
    'ingresos', 'deudaingr', 'deudacred',
    'deudaotro', 'empleo',  'direccion', 'edad'
)

numeric.data <- dataset %>% select(numeric.vars)

kable(head(numeric.data))
```

Un problema que se presenta cuando se trabaja con algoritmos basados
en cálculo de distancias viene producido por el rango de las variables
con las que trabajamos. Si observamos los valores máximos y mínimos
del conjunto de datos de trabajo:

```{r echo=F}
mins <- apply(numeric.data, 2, min)
maxs <- apply(numeric.data, 2, max)
minmax <- rbind(mins, maxs)

rownames(minmax) <- c("Min", "Max")
kable(minmax)
```

Mientras que la variable `ingresos` tiene un rango de más de 400,
otras variables, como `deudacred` apenas tiene un rango de 20
unidades. A la hora de calcular una distancia, la primera variable
tendrá una influencia mucho mayor en el resultados que la
segunda. Para evitar esta problemática, normalizaremos las variables
con las que estamos trabajando, restando a cada elemento su media
y dividiendo por la desviación típica de los datos. De esta manera,
transformamos la distribución de los datos a otra con media nula
y desviación típica unitaria, de forma que todas las variables
serán igualmente relevantes a la hora de calcular la distancia
entre dos elementos de la base de datos:

```{r}
scaled.data <- scale(numeric.data) # La función scale hace la transformación que hemos especificado
```

Una vez aplicada la transformación, aplicamos el algoritmo de
k-medias. Este algoritmo requiere que especifiquemos a priori
el número de _clusters_ en los que se agruparán los datos. Vamos
a tratar de crear dos grupos, para comprobar si con este método
se pueden separar los elementos en las dos clases que se nos
proporcionan:

```{r}
nclust <- 2

kmeans.res <- kmeans(scaled.data, nclust)

autoplot(kmeans.res, data = scaled.data)
```

Podemos comprobar como los dos clusters que se han formado están
significativamente bien separados. Comprobamos si esta separación
se corresponde con los valores de la columna a predecir:

```{r echo=F}
conf.mat <- table(kmeans.res$cluster, dataset$impago)
rownames(conf.mat) <- c(1, 0)
conf.mat <- apply(conf.mat, 2, rev)
kable(conf.mat)
```

Podemos observar que los resultados obtenidos no son especialmente a
la hora de clasificar los elementos de nuestro conjunto de datos en
pagadores y no pagadores. En efecto, si mostramos el mismo gráfico que
anteriormente, pero utilizando la columna impago como etiqueta,
obtenemos el siguiente resultado:

```{r echo=F}
dataset['impago'] <- as.factor(dataset[,'impago'])
autoplot(prcomp(scaled.data), data = dataset, colour = "impago")
```

Donde podemos observar que este algoritmo difícilmente será capaz de
dividir correctamente los datos del conjunto en las dos clases, debido
a que las mismas no forman grupos convexos de elementos. Es posible
que, dado que las dos componentes principales de los datos explican
sólamente el 70 % de la variabilidad de los mismos, al añadir más
componentes las clases sean más fácilmente separables.

Usualmente, cuando se utilizan algoritmos de aprendizaje no
supervisado, no se dispone de una clase a predecir, por lo que no se
puede construir una matriz de confusión como la que hemos mostrado
anteriormente. En ese caso, existen medidas que permiten medir la
calidad de los agrupamientos de forma no supervisada. Algunas de
estas medidas son:

- Medidas de cohesión: Miden cómo de cercanos están los datos dentro
un mismo cluster. Un cluster cuya cohesión es buena suele indicar un
buen agrupamiento, porque esta medida indica que sus elementos son
similares entre si.
- Medidas de separación: Miden la distancia que existe entre dos
clusters distintos. Valores altos en esta medida indican que los
clusters generados están distanciados unos de otros, por lo que los
elementos que los componen están bien diferenciados.
- Medidas de validez: Usualmente, la métrica que se usa para medir la
calidad de un cluster es una combinación de las medidas de separación
y cohesión de sus puntos, y la validez total del agrupamiento es una
combinación de las medidas de validez de los grupos que lo componen.

Una medida que suele ser utilizada para determinar la calidad de un
agrupamiento es lo que se conoce como el coeficiente de silueta. Este
coeficiente se calcula para todos los elementos del conjunto de datos.
Para cada punto:

1. Se calcula la distancia media de cada punto a los elementos de su
grupo ($a_i$)
2. Se calcula la distancia media de cada punto a los elementos que no
son de su grupo ($b_i$)
3. El coeficiente de silueta del elemento $i$ se define como
\[ s_i = \frac{b_i - a_i}{\max{(a_i, b_i)}} \]

Este valor oscila entre -1 y 1. Un valor cercano a 1 es deseable, ya
que indica que el punto está bien representado dentro de su cluster
(cercano a los elementos de su cluster y alejado de los otros grupos).
Un valor negativo, por el contrario, indica que el elemento puede
estar mal emplazado, ya que la distancia media del elemento a los
elementos de su cluster es mayor que la distancia a los puntos de
otros clusters. Una vez calculado el coeficiente de silueta para todos
los puntos de un cluster, el coeficiente de silueta del cluster
completo se calcula como la media de los coeficientes que componen el
grupo, y el coeficiente de silueta del agrupamiento es la media de los
coeficientes de todos los grupos. Podemos calcular los coeficientes
de silueta del agrupamiento que hemos generado anteriormente de la
siguiente forma:

```{r}
scaled.distances <- dist(scaled.data) # Tenemos que calcular
                                      # manualmente las distancias
sil <- silhouette(kmeans.res$cluster, scaled.distances)
plot(sil, col=1:nclust, main = "Silhouette plot")
```

Podemos observar en el gráfico anterior que los resultados no son
especialmente buenos. El cluster etiquetado con el valor 2 tiene un
coeficiente de silueta relativamente alto, pero el cluster marcado en
negro tiene un coeficiente muy cercano a 0, teniendo además una
cantidad importante de puntos con un valor negativo. El coeficiente de
silueta total del agrupamiento podemos observarlo abajo, y tiene un
valor de $0.36$. No es un valor especialmente bueno, lo que podría
estar indicando que el número de clusters que se ha seleccionado no
es el idóneo. Escogimos el valor 2 en un primer momento tratando
de obtener una correlación entre los clusters formados y la clase
a predecir en este conjunto, pero podría no ser la forma idónea
de agrupar los mismos. Repitiendo el proceso anterior cambiando a
3 el número de clusters obtenemos el siguiente resultado:

```{r echo=F}
nclust <- 3

kmeans.res <- kmeans(scaled.data, nclust)

autoplot(kmeans.res, data = scaled.data)

scaled.distances <- dist(scaled.data) # Tenemos que calcular
                                        # manualmente las distancias
sil <- silhouette(kmeans.res$cluster, scaled.distances)
plot(sil, col=1:nclust, main = "Silhouette plot")
```

Observamos que los resultados obtenidos no son especialmente
buenos, siendo de peor calidad que los conseguidos al trabajar
con dos clases. Aumentando el número de clusters, el coeficiente
de silueta empeora:

```{r echo=F}
avgs.sil <- sapply(2:10, function(i){
    kmeans.res <- kmeans(scaled.data, i)
    cluster.stats(dist(scaled.data), kmeans.res$cluster)$avg.silwidth
})

avgs.sil <- as.matrix(t(avgs.sil))

colnames(avgs.sil) <- 2:10
rownames(avgs.sil) <- "Sil. coef"
kable(avgs.sil, digits=3)
```

Como podemos observar en la tabla anterior, el mejor resultado,
medido utilizando el coeficiente de silueta, se obtiene con dos
clusters, aunque no se produce un resultado especialmente bueno.

## Adición de variables binarias

En el apartado anterior, sólo hemos utilizado l
```{r}
numeric.vars <- c(
    'ingresos', 'deudaingr', 'deudacred',
    'deudaotro', 'empleo',  'direccion', 'edad'
)

nominal.vars <- c(
    'educ', 'impago'
)
```

## K-medias

```{r}
nclust <- 3

dataset <- read.csv(dataset.path, sep = ";", dec=",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)

nominal.data <- dataset %>% select(nominal.vars)
nominal.data <- dummy_cols(nominal.data, select_columns = 'educ')

numeric.distances <- dist(numeric.data)

nominal.distances <- dist(nominal.data, method="binary")

final.distances <- (
    length(numeric.vars)*numeric.distances + length(nominal.vars)*nominal.distances
) / (length(numeric.vars) + length(nominal.vars))

kmeans.result <- kmeans(final.distances, nclust)

idx <- sample(1: dim(dataset)[1], 300)

plotcluster(dataset, kmeans.result$cluster)

d1 <- dist(numeric.data[idx,])
d2 <- dist(nominal.data[idx,], method="binary")

d <- (d1+d2)/2

sil <- silhouette(kmeans.result$cluster, final.distances)
plot(sil, col = 1:nclust)

table(kmeans.result$cluster, dataset$impago)

cluster.stats(final.distances, kmeans.result$cluster, alt.clustering = dataset$impago + 1)
```

### Normalización de variables

```{r}
dataset <- read.csv(dataset.path, sep=";",  dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

nominal.data <- dummy_cols(nominal.data, select_columns = "educ",
                           remove_selected_columns = T)

## Normalizamos los atributos continuos
## Esta función nos estandariza los valores del dataframe por columnas
numeric.data <- numeric.data %>% select(
                                     "ingresos", "deudaingr", "empleo", "edad"
                                 )
scaled.data <- scale(numeric.data)

## Comprobamos que en efecto las columnas están normalizadas
apply(scaled.data, 2, mean) # Los valores son muy cercanos a cero
apply(scaled.data, 2, sd)

scaled.distances <- dist(scaled.data)

binary.distances <- dist(nominal.data, method="binary")

final.distances <- (scaled.distances + binary.distances) / 2

final.distances <- scaled.distances

kmeans.result <- kmeans(final.distances, nclust)

idx <- sample(1: dim(dataset)[1], 300)

plotcluster(dataset, kmeans.result$cluster)

d1 <- dist(scaled.data[idx,])
d2 <- dist(nominal.data[idx,], method="binary")

d <- (d1+d2)/2

sil <- silhouette(kmeans.result$cluster, final.distances)
plot(sil, col = 1:nclust)

cluster.stats(final.distances, kmeans.result$cluster)
```

## DBSCAN

```{r}
dataset <- read.csv(dataset.path, sep = ";", dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

scaled.data <- scale(numeric.data)

scaled.distances <- dist(scaled.data)
nominal.distances <- dist(nominal.data, method="binary")

final.distances <- (scaled.distances + binary.distances) / 2

## Aplicamos DBSCAN (con el parámetro dist este método acepta una matriz
## de distancias en lugar de los datos)
dbscan.res <- dbscan(final.distances, eps=.3, MinPts=5, method="dist")

fviz_cluster(dbscan.res, scaled.data)

plotcluster(dataset, dbscan.res$cluster)

sil <- silhouette(dbscan.res$cluster, final.distances)

plot(sil, col=length(unique(dbscan.res$cluster)))
```

## Clustering jerárquico

```{r}
dataset <- read.csv(dataset.path, sep=";",  dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

numeric.distances <- dist(numeric.data)
nominal.distances <- dist(nominal.data, method="binary")
final.distances <- (numeric.distances + nominal.distances) / 2

hclust <- hclust(final.distances, method="ward.D2")

hclust

plot(hclust)

rect.hclust(hclust, k=nclust)

groups <- cutree(hclust, k=3)

groups

plotcluster(dataset, groups)

sil <- silhouette(groups, final.distances)

plot(sil, col <- nclust)

cluster.stats(final.distances, groups)
```

### Normalización de variables

```{r}
dataset <- read.csv(dataset.path, sep=";",  dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

scaled.data <- scale(numeric.data)

scaled.distances <- dist(scaled.data)
nominal.distances <- dist(nominal.data, method="binary")
final.distances <- (scaled.distances + nominal.distances) / 2

hclust <- hclust(final.distances, method="ward.D2")

hclust

plot(hclust)

rect.hclust(hclust, k=3)

groups <- cutree(hclust, k=3)

groups

plotcluster(dataset, groups)

sil <- silhouette(groups, final.distances)

plot(sil, col <- nclust)

cluster.stats(final.distances, groups)
```

## k-medioides

```{r}
dataset <- read.csv(dataset.path, sep=";",  dec=",")
dataset <- dataset[1:700,]


numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

numeric.distances <- dist(numeric.data)
nominal.distances <- dist(nominal.data, method="binary")
final.distances <- (numeric.distances + nominal.distances) / 2

pam.result <- pam(final.distances, 3)

idx <- sample(1:dim(dataset)[1], 200)

clusters <- pam.result$cluster

plotcluster(dataset[idx,], clusters[idx])

scaled.d <- dist(scaled.data[idx,])
binary.d <- dist(nominal.data[idx,], method="binary")

d <- (scaled.d + binary.d) / 2
sil <- silhouette(clusters[idx], d)

plot(sil, col=1:3)

cluster.stats(final.distances, clusters)$avg.silwidth
```
### Resultados con normalización de variables

```{r}
dataset <- read.csv(dataset.path, sep=";", dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

scaled.data <- scale(numeric.data)

scaled.distances <- dist(scaled.data)
nominal.distances <- dist(nominal.data, method="binary")
final.distances <- (scaled.distances + nominal.distances) / 2

pam.result <- pam(final.distances, 3)

idx <- sample(1:dim(dataset)[1], 200)

clusters <- pam.result$cluster

plotcluster(dataset[idx,], clusters[idx])

scaled.d <- dist(scaled.data[idx,])
binary.d <- dist(nominal.data[idx,], method="binary")

d <- (scaled.d + binary.d) / 2
sil <- silhouette(clusters[idx], d)

plot(sil, col=1:3)

cluster.stats(final.distances, clusters)$avg.silwidth
```

### Búsqueda del valor óptimo de k

```{r}
dataset <- read.csv(dataset.path, sep=";", dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

scaled.data <- scale(numeric.data)

scaled.distances <- dist(scaled.data)
nominal.distances <- dist(nominal.data, method="binary")

final.distances <- (scaled.distances + nominal.distances) / 2

pamk.result <- pamk(scaled.distances)

plot(pamk.result$pamobject)

cluster.stats(final.distances, pamk.result$pamobject$clustering)
```

## Fuzzy k-means

```{r}
dataset <- read.csv(dataset.path, sep=";", dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

scaled.data <- scale(numeric.data)

scaled.distances <- dist(scaled.data)
nominal.distances <- dist(nominal.data, method="binary")

final.distances <- (scaled.distances + nominal.distances) / 2

fuzzy.result <- fanny(final.distances, 3, memb.exp=1.3)

plot(fuzzy.result)

str(fuzzy.result)

cluster.stats(final.distances, fuzzy.result$clustering)
```
