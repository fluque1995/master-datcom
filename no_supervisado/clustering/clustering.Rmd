---
title: "Clustering"
subtitle: "Minería de datos: aprendizaje no supervisado y detección de anomalías"
author: "Francisco Luque Sánchez"
date: "21/12/2019"
titlepage: true
titlepage-background: "background.pdf"
headrule-color: "435488"
urlcolor: 'blue'
output:
    pdf_document:
        number_sections: yes
        template: eisvogel
bibliography: references.bib
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE)
set.seed(0)
library(ggplot2)
library(stats)
library(fpc)
library(cluster)
library(dplyr)
library(factoextra)
library(fastDummies)
```

# Introducción

En este trabajo se van a estudiar técnicas de agrupamiento de ejemplos
de una base de datos desde el punto de vista del aprendizaje no
supervisado. En el paradigma de aprendizaje no supervisado no se
dispone de un conjunto de etiquetas o categorías en las que agrupar
los datos y que son conocidas a priori, si no que se tratan de formar
grupos de elementos (_clusters_) en función de la similaridad que hay
entre ellos. Esta similaridad está usualmente definida en función de la
distancia que existe entre ellos dentro del espacio de
características. Cuanto menor sea la distancia entre dos elementos de
nuestro conjunto, más probable es que exista alguna relación entre los
mismos, y por tanto será deseable que el algoritmo de agrupamiento los
coloque dentro del mismo grupo. Por el contrario, cuanto más alejados
estén los elementos en el espacio de características, más diferencia
existirá entre ellos, y por tanto nos interesará que se emplacen en
_clusters_ diferentes.

Por tanto, para tener bien definido un algoritmo de agrupamiento,
tendremos que definir dos cosas. Por un lado, habrá que establecer una
medida de distancia o similaridad entre cualesquiera dos elementos de
nuestro conjunto de datos. Esta distancia dependerá del tipo de datos
con los que estemos trabajando. Si los datos con los que estamos
trabajando están compuestos por atributos numéricos continuos, la
distancia a utilizar será usualmente la distancia euclídea, o una
distancia de Minkowski (la distancia euclídea es un caso particular de
una distancia de Minkowski, cuando k=2). En cambio, si la información
está codificada como un conjunto de variables binarias o nominales, es
posible que estas distancias no estén bien definidas, o no tengan
sentido, y habrá que recurrir a otras funciones de distancia. Otros
tipos de datos, como textos, imágenes o series temporales, necesitarán
también de medidas de distancia propias que nos permitan comparar dos
elementos. En el desarrollo de esta práctica se estudiará cómo el uso
de distintas medidas de distancia pueden dar lugar a resultados
distintos. Por otro lado, habrá de definir qué estrategia se utiliza
para agrupar los puntos una vez que conocemos la distancia que existe
entre ellos. En esta práctica se estudiarán distintas políticas de
agrupamiento, concretamente las siguientes:

- k-means
- DBSCAN
- Clustering jerárquico
- k-medioides
- k-means difuso

Para cada algoritmo de los dictados anteriormente, se hará una breve
descripción teórica del mismo, y se harán pruebas para observar los
resultados que obtiene el mismo sobre un determinado dataset. Además,
para algunos de los algoritmos, estudiaremos cómo el cambio en la
distancia utilizada produce distintos resultados.

## Conjunto de datos empleado - Bankloan

El conjunto de datos sobre el que trabajaremos tiene el nombre de
Bankloan dataset. Es uno de los conjuntos de datos de ejemplo que
se distribuyen con el software IBM SPSS [@ibm]

```{r}
dataset.path <- "dataset/bankloan-spss.csv"

dataset <- read.csv(dataset.path, sep = ";", dec=",")

dataset <- dataset[1:700,]

numeric.vars <- c(
    'ingresos', 'deudaingr', 'deudacred',
    'deudaotro', 'empleo',  'direccion', 'edad'
)

nominal.vars <- c(
    'educ', 'impago'
)
```

## K-medias

```{r}
nclust <- 3

dataset <- read.csv(dataset.path, sep = ";", dec=",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)

nominal.data <- dataset %>% select(nominal.vars)
nominal.data <- dummy_cols(nominal.data, select_columns = 'educ')

numeric.distances <- dist(numeric.data)

nominal.distances <- dist(nominal.data, method="binary")

final.distances <- (
    length(numeric.vars)*numeric.distances + length(nominal.vars)*nominal.distances
) / (length(numeric.vars) + length(nominal.vars))

kmeans.result <- kmeans(final.distances, nclust)

idx <- sample(1: dim(dataset)[1], 300)

plotcluster(dataset, kmeans.result$cluster)

d1 <- dist(numeric.data[idx,])
d2 <- dist(nominal.data[idx,], method="binary")

d <- (d1+d2)/2

sil <- silhouette(kmeans.result$cluster, final.distances)
plot(sil, col = 1:nclust)

table(kmeans.result$cluster, dataset$impago)

cluster.stats(final.distances, kmeans.result$cluster, alt.clustering = dataset$impago + 1)
```

### Normalización de variables

```{r}
dataset <- read.csv(dataset.path, sep=";",  dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

nominal.data <- dummy_cols(nominal.data, select_columns = "educ",
                           remove_selected_columns = T)

## Normalizamos los atributos continuos
## Esta función nos estandariza los valores del dataframe por columnas
numeric.data <- numeric.data %>% select(
                                     "ingresos", "deudaingr", "empleo", "edad"
                                 )
scaled.data <- scale(numeric.data)

## Comprobamos que en efecto las columnas están normalizadas
apply(scaled.data, 2, mean) # Los valores son muy cercanos a cero
apply(scaled.data, 2, sd)

scaled.distances <- dist(scaled.data)

binary.distances <- dist(nominal.data, method="binary")

final.distances <- (scaled.distances + binary.distances) / 2

final.distances <- scaled.distances

kmeans.result <- kmeans(final.distances, nclust)

idx <- sample(1: dim(dataset)[1], 300)

plotcluster(dataset, kmeans.result$cluster)

d1 <- dist(scaled.data[idx,])
d2 <- dist(nominal.data[idx,], method="binary")

d <- (d1+d2)/2

sil <- silhouette(kmeans.result$cluster, final.distances)
plot(sil, col = 1:nclust)

cluster.stats(final.distances, kmeans.result$cluster)
```

## DBSCAN

```{r}
dataset <- read.csv(dataset.path, sep = ";", dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

scaled.data <- scale(numeric.data)

scaled.distances <- dist(scaled.data)
nominal.distances <- dist(nominal.data, method="binary")

final.distances <- (scaled.distances + binary.distances) / 2

## Aplicamos DBSCAN (con el parámetro dist este método acepta una matriz
## de distancias en lugar de los datos)
dbscan.res <- dbscan(final.distances, eps=.3, MinPts=5, method="dist")

fviz_cluster(dbscan.res, scaled.data)

plotcluster(dataset, dbscan.res$cluster)

sil <- silhouette(dbscan.res$cluster, final.distances)

plot(sil, col=length(unique(dbscan.res$cluster)))
```

## Clustering jerárquico

```{r}
dataset <- read.csv(dataset.path, sep=";",  dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

numeric.distances <- dist(numeric.data)
nominal.distances <- dist(nominal.data, method="binary")
final.distances <- (numeric.distances + nominal.distances) / 2

hclust <- hclust(final.distances, method="ward.D2")

hclust

plot(hclust)

rect.hclust(hclust, k=nclust)

groups <- cutree(hclust, k=3)

groups

plotcluster(dataset, groups)

sil <- silhouette(groups, final.distances)

plot(sil, col <- nclust)

cluster.stats(final.distances, groups)
```

### Normalización de variables

```{r}
dataset <- read.csv(dataset.path, sep=";",  dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

scaled.data <- scale(numeric.data)

scaled.distances <- dist(scaled.data)
nominal.distances <- dist(nominal.data, method="binary")
final.distances <- (scaled.distances + nominal.distances) / 2

hclust <- hclust(final.distances, method="ward.D2")

hclust

plot(hclust)

rect.hclust(hclust, k=3)

groups <- cutree(hclust, k=3)

groups

plotcluster(dataset, groups)

sil <- silhouette(groups, final.distances)

plot(sil, col <- nclust)

cluster.stats(final.distances, groups)
```

## k-medioides

```{r}
dataset <- read.csv(dataset.path, sep=";",  dec=",")
dataset <- dataset[1:700,]


numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

numeric.distances <- dist(numeric.data)
nominal.distances <- dist(nominal.data, method="binary")
final.distances <- (numeric.distances + nominal.distances) / 2

pam.result <- pam(final.distances, 3)

idx <- sample(1:dim(dataset)[1], 200)

clusters <- pam.result$cluster

plotcluster(dataset[idx,], clusters[idx])

scaled.d <- dist(scaled.data[idx,])
binary.d <- dist(nominal.data[idx,], method="binary")

d <- (scaled.d + binary.d) / 2
sil <- silhouette(clusters[idx], d)

plot(sil, col=1:3)

cluster.stats(final.distances, clusters)$avg.silwidth
```
### Resultados con normalización de variables

```{r}
dataset <- read.csv(dataset.path, sep=";", dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

scaled.data <- scale(numeric.data)

scaled.distances <- dist(scaled.data)
nominal.distances <- dist(nominal.data, method="binary")
final.distances <- (scaled.distances + nominal.distances) / 2

pam.result <- pam(final.distances, 3)

idx <- sample(1:dim(dataset)[1], 200)

clusters <- pam.result$cluster

plotcluster(dataset[idx,], clusters[idx])

scaled.d <- dist(scaled.data[idx,])
binary.d <- dist(nominal.data[idx,], method="binary")

d <- (scaled.d + binary.d) / 2
sil <- silhouette(clusters[idx], d)

plot(sil, col=1:3)

cluster.stats(final.distances, clusters)$avg.silwidth
```

### Búsqueda del valor óptimo de k

```{r}
dataset <- read.csv(dataset.path, sep=";", dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

scaled.data <- scale(numeric.data)

scaled.distances <- dist(scaled.data)
nominal.distances <- dist(nominal.data, method="binary")

final.distances <- (scaled.distances + nominal.distances) / 2

pamk.result <- pamk(scaled.distances)

plot(pamk.result$pamobject)

cluster.stats(final.distances, pamk.result$pamobject$clustering)
```

## Fuzzy k-means

```{r}
dataset <- read.csv(dataset.path, sep=";", dec = ",")
dataset <- dataset[1:700,]

numeric.data <- dataset %>% select(numeric.vars)
nominal.data <- dataset %>% select(nominal.vars)

scaled.data <- scale(numeric.data)

scaled.distances <- dist(scaled.data)
nominal.distances <- dist(nominal.data, method="binary")

final.distances <- (scaled.distances + nominal.distances) / 2

fuzzy.result <- fanny(final.distances, 3, memb.exp=1.3)

plot(fuzzy.result)

str(fuzzy.result)

cluster.stats(final.distances, fuzzy.result$clustering)
```
