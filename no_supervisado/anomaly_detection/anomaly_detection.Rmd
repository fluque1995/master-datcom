---
title: "Detección de anomalías"
subtitle: "Minería de datos: aprendizaje no supervisado y detección de anomalías"
author: "Francisco Luque Sánchez"
date: "21/12/2019"
titlepage: true
titlepage-background: "background.pdf"
headrule-color: "435488"
urlcolor: 'blue'
script-font-size: \scriptsize
nncode-block-font-size: \scriptsize
output:
    pdf_document:
        number_sections: yes
        template: eisvogel
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = FALSE)
set.seed(0)
library(ggplot2)
library(knitr)
library(rmatio)
library(GGally)
library(gridExtra)
library(ggfortify)

## Cargamos las librerías y funciones pertinentes
source("Outliers_A2_Librerias_a_cargar_en_cada_sesion.R")

source("Outliers_A3_Funciones_a_cargar_en_cada_sesion.R")
```

# Introducción

En este trabajo se va a desarrollar una práctica sobre detección de
ejemplos anómalos en un conjunto de datos. Se define como dato anómalo
aquel elemento de un conjunto de datos cuyo comportamiento difiere del
comportamiento esperable. Este comportamiento se traduce en que los
valores registrados para alguna de las variables se sale de los
valores esperados. En función de la naturaleza de la desviación,
podremos tener distintos tipos de comportamientos anómalos:

- Ejemplos que presentan un valor extremo para alguno de los atributos
medidos.
- Ejemplos que presentan una combinación de valores anormal para
varias columnas estudiadas en conjunto, pero con valores comunes si
son estudiados individualmente.

Estudiaremos distintas técnicas de detección de datos anómalos, tanto
del primer como del segundo tipo.

# IQR

El primer método que estudiaremos está basado en el rango
intercuartílico.  Este método trabaja de forma univariante, tratando
todas las variables del conjunto por separado. Por tanto, nos
permitirá detectar valores extremos en las variables por separado,
pero no nos servirá para detectar combinaciones atípicas de valores.
El funcionamiento del test se basa en el estudio de los cuartiles de
una distribución normal. Cuando se trabaja con variables normalmente
distribuidas, un enfoque típico para la detección de anomalías
consiste en considerar como anómalos aquellos datos que se salen del
intervalo $(\mu - k\sigma, \mu + k\sigma)$, donde $\mu$ y $\sigma$ son
la media y la desviación típica de la distribución,
respectivamente. En función del valor de $k$ que tomemos, cubriremos a
un mayor número de puntos de la distribución normal. En concreto,
suele tomarse el intervalo con $k=2$, de forma que aproximadamente el
95 % de los valores de la distribución caen en el intervalo, o $k=3$,
intervalo en el que se encuentran el 99.7 % de los valores. Los valores
de la distribución que quedan fuera de estos límites se consideran datos
atípicos

No obstante, la asunción de normalidad en los datos es una suposición
fuerte, que usualmente no se cumple. Además la media y la desviación
típica de los datos son dos estadísticos muy sensibles a la existencia
de valores anómalos. De esta forma, en lugar de utilizar el método
descrito anteriormente, resulta más útil comparar el comportamiento
del rango intercuartílico con el de la desviación, y utilizar el
primer estadístico, que es más robusto ante la existencia de valores
atípicos.  Para la distribución normal, el primer cuartil está en el
valor $-0.67$, y el tercer cuartil en el valor $0.67$. El rango
intercuartílico es, por tanto $1.34$, Tomando entonces el valor $k' =
1.5$, tenemos que el intervalo $(-1.5*IQR, 1.5*IQR)$ contiene
aproximadamente los mismos valores que el intervalo $(-2\sigma,
2\sigma)$.

Utilizando esta justificación, el método IQR etiqueta como datos
anómalos normales para una variable aquellos que se desvían de la
media más de 1.5 por el valor del rango intercuartílico. Por un
razonamiento similar, se marcan como datos anómalos extremos aquellos
que se desvían de la media más de 3*IQR.

## Aplicación del algoritmo

Pasamos a aplicar este método para detectar outliers en nuestro
conjunto de datos. Mostraremos en primer lugar el proceso completo
para una variable, y lo aplicaremos después para todas las demás:

```{r, echo=T, fig.height=4}
## Leemos el dataset y seleccionamos una columna
# dataset <- read.csv("dataset/ecoli.data")
dataset <- as.data.frame(read.mat("dataset/vertebral.mat")$X)
columna.scaled <- scale(dataset$V5)

## Calculamos los cuartiles y el IQR
cuartil.primero <- quantile(columna.scaled, .25)
cuartil.tercero <- quantile(columna.scaled, .75)
iqr             <- cuartil.tercero - cuartil.primero

## Calculamos los valores a partir de los cuales se consideran los outliers
extremo.superior.outlier.normal <- cuartil.tercero + 1.5*iqr
extremo.inferior.outlier.normal <- cuartil.primero - 1.5*iqr
extremo.superior.outlier.extremo <- cuartil.tercero + 3*iqr
extremo.inferior.outlier.extremo <- cuartil.primero - 3*iqr

## Construimos los vectores que nos determinan los outliers
vector.es.outlier.normal <- (
    (columna.scaled > extremo.superior.outlier.normal &
     columna.scaled < extremo.superior.outlier.extremo) |
    (columna.scaled < extremo.inferior.outlier.normal &
     columna.scaled > extremo.inferior.outlier.extremo)
)

vector.es.outlier.extremo <- (
    columna.scaled < extremo.inferior.outlier.extremo |
    columna.scaled > extremo.superior.outlier.extremo
)

## Calculamos los valores normales y extremos
claves.outliers.normales <- which(vector.es.outlier.normal)
data.frame.outliers.normales <- dataset[claves.outliers.normales,]
valores.outliers.normales <- data.frame.outliers.normales["V5"]

claves.outliers.extremos <- which(vector.es.outlier.extremo)
data.frame.outliers.extremos <- dataset[claves.outliers.extremos,]
valores.outliers.extremos <- data.frame.outliers.extremos['V5']

valores.normalizados.outliers.normales <- columna.scaled[claves.outliers.normales]

par(mfrow=c(1,2))
## Mostramos gráficamente los outliers
MiPlot_Univariate_Outliers(columna.scaled, claves.outliers.normales,
                           "Outliers normales")
MiPlot_Univariate_Outliers(columna.scaled, claves.outliers.extremos,
                           "Outliers extremos")
```

Una vez hemos visto cómo se aplica el método a una determinada columna, nos
interesará aplicarlo a todas las variables de nuestro conjunto (ocultamos
el código porque no aporta nueva información). Mostramos gráficamente los
resultados:

```{r, fig.height=3.4}
## Repetimos la operacieón con una función del fichero A2
indices.de.outliers.en.alguna.columna <- vector_claves_outliers_IQR_en_alguna_columna(dataset)

## Calculamos el número de outliers totales por columna
frame.es.outlier <- sapply(
    1:ncol(dataset), vector_es_outlier_IQR, datos = dataset
)

frame.es.outlier.extremo <- sapply(
    1:ncol(dataset), vector_es_outlier_IQR, datos = dataset, coef = 3
)

frame.es.outlier.normal <- (frame.es.outlier & !frame.es.outlier.extremo)

## Mostramos gráficamente los outliers
normal.outliers <- lapply(1:ncol(dataset), function(x) {
    MiPlot_Univariate_Outliers(dataset[,x],
                               frame.es.outlier.normal[,x],
                               paste("Outliers normales - columna", x))
})
extreme.outliers <- lapply(1:ncol(dataset), function(x) {
    MiPlot_Univariate_Outliers(dataset[,x],
                               frame.es.outlier.extremo[,x],
                               paste("Outliers extremos - columna", x))
})

foo <-sapply(1:ncol(dataset), function(x) grid.arrange(
                                              normal.outliers[[x]],
                                              extreme.outliers[[x]], ncol=2)
             )

numero.total.outliers.por.columna <- apply(frame.es.outlier, 2, sum)

kable(as.data.frame(t(numero.total.outliers.por.columna)),
      col.names = c("Columna 1", "Columna 2", "Columna 3",
                    "Columna 4", "Columna 5", "Columna 6"),
      caption = "Número de outliers en cada columna del conjunto de datos"
      )
```

Se pueden extraer varias conclusiones de las gráficas y la tabla
anteriores. En primer lugar, tenemos que la variable primera no tiene
outliers de ningún tipo, mientras que en el resto de variables se
tiene un número bastante considerable. El conjunto tiene unos 3800
elementos, así que 400 outliers, como llega a tener la variables 2,
suponen más del 10 % de los valores, lo cual es un número anormalmente
alto. Un comportamiento curioso que se puede observar también es la
distribución de los outliers. En todas las variables ocurre que los
outliers se concentran en los valores altos. Esto nos indica que
la distribuciones están fuertemente sesgadas a la derecha. El
caso más extremo lo tenemos para la segunda variable, como podemos
observar a continuación:

```{r}
ggplot(dataset, aes(x=V2)) + geom_histogram(bins=10)
```

Donde tenemos el grueso de los datos agrupado en la parte izquierda
del gráfico (cercanos a 0) y un pequeño grupo de valores dispersos en
la parte derecha. Sería interesante conocer el origen de las
variables, para intentar buscar las causas que producen estas
desviaciones, pero no se nos proporciona dicha información con el
conjunto de datos.

Una vez hemos visto cómo podemos detectar valores anómalos utilizando
el método del rango intercuartílico, vamos a ver cómo podemos dotar
de más robustez a la detección de anomalías univariantes.

# Tests estadísticos para la detección de anomalías univariables

En este apartado veremos cómo podemos aplicar tests estadísticos para
la detección de datos anómalos univariantes. En primer lugar,
aplicaremos el test de Grubbs, el cual nos permite saber si existe un
único outlier en una determinada columna del conjunto de datos, y
posteriormente utilizaremos el test de Rosner, el cual nos permitirá
deducir si hay hasta $k$ outliers, para k conocido de antemano.
Comenzamos comentando el test de Grubbs.

## Test de Grubbs

Este test estadístico permite encontrar outliers en una distribución
univariante, asumiendo que existe normalidad en la misma. Existen dos
versiones del test, dependiendo si hacemos el test con una o dos
colas.  La versión del test de una cola nos permite saber si el máximo
o el mínimo de los valores es anómalo, pero no trabaja sobre ambos
valores simultáneamente. Nosotros utilizaremos el test de dos colas,
que trabaja sobre toda la muestra. Para ambas versiones del test, las
hipótesis que plantea el mismo son las siguientes.

- $H_0$: No hay anomalías en el conjunto de datos
- $H_1$: Hay exactamente una anomalía en el conjunto de datos

El estadístico del test de Grubbs se define como:

\[
G = \frac{\max\limits_{i=1,..., N} \lvert Y_i - \bar{Y} \rvert}{S}
\]

Donde $\bar{Y}$ y $S$ representan la media y la desviación típica
muestrales, simultáneamente. Para el test de dos colas, la hipótesis
nula se rechaza para un nivel de significación $\alpha$ si el
estadístico $G$ cumple:

\[ G>{\frac {N-1}{{\sqrt {N}}}}{\sqrt {{\frac {t_{{\alpha
/(2N),N-2}}^{2}}{N-2+t_{{\alpha /(2N),N-2}}^{2}}}}} \]

Donde $t_{{\alpha/(2N),N-2}}$ es el valor crítico para el nivel de
significación $\alpha/2N$ de una distribución T de Student con $N-2$
grados de libertad. Vamos a ver cómo podemos aplicar este test a una
columna de nuestro conjunto de datos.

```{r, echo=T}
selected.col <- dataset$V4

## Dado que el p-valor es inferior a 0.05, tenemos que el valor de mayor
## dispersión respecto de la media es un valor anómalo
grubbs.test(selected.col, two.sided = T)
indice.de.outlier.Grubbs <- order(abs(selected.col - mean(selected.col)),
                                  decreasing = T)[1]
valor.de.outlier.Grubbs <- selected.col[indice.de.outlier.Grubbs]
MiPlot_Univariate_Outliers(selected.col, indice.de.outlier.Grubbs,
                           "Valor outlier según el test de Grubbs")
```

Podemos observar que, para esta variable, se detecta que el valor de
máxima varianza es un valor anómalo. Este test nos permite averiguar,
precisamente, si el valor de máxima varianza se desvía anormalmente
del resto del conjunto de datos. No obstante, ofrece varios problemas.
En primer lugar, sólo nos permite averiguar si hay un valor anómalo,
pero no nos aporta información del resto de puntos. Por ejemplo, en el
caso anterior, el punto que se encuentra cercano al marcado como
outlier, y que presenta el mismo valor de $y$, es claramente otro
punto anómalo. Además de este, probablemente el punto aislado que se
muestra a la derecha también tenga un valor anormalmente alto. Para
estos dos puntos, el test de Grubbs no aporta información. Una posible
forma de solventar esta problemática consiste en ejecutar
iterativamente este algoritmo mientras se sigan encontrando anomalías,
quitando cada vez el punto marcado como anomalía. No obstante, el
hecho de trabajar con un único punto puede producir ciertos
problemas. Por ejemplo existe una problemática conocida como masking,
en la cual la presencia de varios valores anómalos hacen que este test
no permita rechazar la hipótesis nula. Ponemos a continuación un
ejemplo de masking:

```{r}
## Tenemos dos valores claramente anómalos
datos.masking = c(45,56,54,34,32,45,67,45,67,154,125,65)
plot(datos.masking)
grubbs.test(datos.masking, two.sided=T)
```

Obtenemos un p-valor superior a 0.05, por lo que con los niveles de
significación estándar, no podemos rechazar la hipótesis nula y por
tanto no tendríamos ningún outlier. No obstante, podemos observar que
existen dos valores que se desvían significativamente de los valores
normales, pero su presencia hace que la desviación típica del conjunto
de datos aumente, y por tanto este método no detecte su presencia.  En
el siguiente apartado veremos otro text estadístico, que nos permitirá
afrontar esta problemática.

## Test de Rosner

Para solucionar el problema anterior, suelen utilizarse otros tests
estadísticos, que permiten averiguar si hay un número $k$ (fijo de
antemano) de outliers en la muestra. No obstante, estos métodos
carecen de mucho interés, ya que existen tests más informativos, que
nos permiten discernir si existen un número de outliers menor o igual
al número $k$ fijado de antemano, en lugar de necesitar el número
exacto de outliers. Entre estos tests se encuentra el test de Rosner.
Este test trabaja de la siguiente manera. Se calcula la serie de
estadísticos siguiente:

\[
\large R_{i+1} = \frac{|x^{(i)} - \bar x^{(i)}|}{s^{(i)}}
\]

Donde $\bar{x}^{(i)}$ y $\bar{s}^{(i)}$ son la media y la desviación
típica del conjunto tras eliminar los $i$ valores más extremos, y
$x^{(i)}$ es la observación de dicho subconjunto que más se desvía de
la media. Una vez los valores $R_{1}, ..., R_k$ se han calculado, se
realizan una serie de tests de hipótesis consistentes en comparar los
valores $R_i$ con una serie de valores $\lambda_i$, los cuales se
calculan como

\[
\lambda_{i+1} = \frac{t_{p, n-i-2} (n-i-1)}{\sqrt{(n-i-2 + t_{p, n-i-2}) (n-i)}}
\]

Donde de nuevo, $t_{p,n}$ corresponde al percentil $p$ extraído de una
distribución t de Student con $n$ grados de libertad.

Para ver cómo funciona el algoritmo sobre nuestros datos, trataremos
de encontrar los outliers por este método sobre los datos sintéticos
que hemos generado previamente:

```{r}
test.de.rosner <- rosnerTest(datos.masking, 3)

test.de.rosner$all.stats$Outlier
test.de.rosner$all.stats$Obs.Num

outliers.rosner <- test.de.rosner$all.stats$Obs.Num[
                                                test.de.rosner$all.stats$Outlier
                                            ]
MiPlot_Univariate_Outliers(datos.masking, outliers.rosner,
                           "Outliers detectados por el test de Rosner")
```

En nuestro caso, hemos ejecutado el test para buscar un máximo de 3
outliers, pero sólo han dado positivo dos de ellos (los cuales
habíamos observado a simple vista a priori). La ventaja que aporta
este test frente al de Grubbs es la capacidad de detectar más de un
elemento anómalo al mismo tiempo. Aplicamos a continuación este test a
nuestro conjunto de datos, para tratar de detectar valores anómalos
sobre nuestras variables.

```{r}
apply(dataset, 2, MiPlot_resultados_TestRosner, num_outliers=10)
```

# Test de hipótesis para detección de anomalías multivariables

Una vez que hemos visto tests de hipótesis que nos permiten detectar
outliers en variables unidimensionales, lo que nos será realmente
interesante es encontrar valores anómalos en distribuciones de mayor
dimensionalidad. En este caso no buscaremos valores anormales dentro
de una única variable, si no que la idea será encontrar combinaciones
atípicas de valores en varias variables. De esta forma, aunque los
valores que toma cada variable por separado puedan ser valores
normales, la anomalía proviene de la combinación de los mismos.

Vamos a aplicar este test sobre nuestro conjunto de datos. Para tratar
de dar una información lo más visual posible, trabajaremos en primer
lugar sobre dos columnas del dataset, en lugar del conjunto de datos
completo. De esta forma, podremos observar gráficamente si somos
capaces de detectar outliers bivariantes que no son considerados como
tales cuando trabajamos con las variables de forma independiente.Este
test tiene dos posibles formas de aplicación, para detectar un único
outlier, o para detectar todos los posibles outliers del conjunto de
datos. En el primero de los casos, las hipótesis que se imponen son

- $H_0$: en el conjunto de datos no hay ningún outlier.
- $H_1$: en el conjunto de datos hay exactamente un outlier.

Usando este test, sólo podremos sacar conclusiones sobre el valor más
desviado de nuestra distribución. En el segundo de los casos, se
relaja la hipótesis alternativa, lo cual nos obliga a aplicar una
corrección sobre el nivel de significación, para evitar incurrir en un
error FWER. La nueva hipótesis alternativa es la siguiente:

- $H_1$: en el conjunto de datos hay al menos un outlier

Aplicamos ambos métodos sobre las columnas 3 y 4 de
nuestro dataset. Comenzamos por el primero de los métodos:

```{r}
bivar.dataset <- dataset[, 3:4]
bivar.dataset.scaled <- scale(bivar.dataset)
nivel.de.significacion = 0.05

## Método de Cerioli para el cálculo de outliers
cerioli <- cerioli2010.fsrmcd.test(
    bivar.dataset, signif.alpha = nivel.de.significacion
)
is.outlier.cerioli <- which(cerioli$outliers)
dist.mah.ponderadas <- cerioli$mahdist.rw
order.idx <- order(dist.mah.ponderadas, decreasing = T)

## Estudiamos únicamente el primero de los outliers, aquel que tiene
## una distancia de Mahalanobis mayor

kable(t(bivar.dataset.scaled[order.idx[1],]),
      caption = "Valores normalizados para el outlier encontrado con el test de Cerioli de tipo a")


ggplot(bivar.dataset, aes(x=V3, y=V4)) +
    geom_point(colour = "lightblue") +
    geom_point(data=bivar.dataset[order.idx[1],], colour = "red")
```

En el caso del primer outlier, podemos comprobar que tiene un valor
anormalmente alto para la variable V4, por lo que en realidad no es uno
de esos puntos que estamos buscando, que tienen valores normales en
ambas variables pero su combinación es anormal. No obstante, si en
lugar de utilizar el test como un test de tipo a lo utilizamos como un
test de tipo b, corrigiendo adecuadamente el nivel de significación,
lo que obtenemos es lo siguiente:

```{r}
bivar.dataset <- dataset[, 3:4]
bivar.dataset.scaled <- scale(bivar.dataset)
nivel.de.significacion.penalizado = 1 - (1 - nivel.de.significacion) ^ (1 / nrow(bivar.dataset))

## Método de Cerioli para el cálculo de outliers
cerioli <- cerioli2010.fsrmcd.test(
    bivar.dataset, signif.alpha = nivel.de.significacion.penalizado
)

is.outlier.cerioli <- which(cerioli$outliers)
cerioli.outliers <- bivar.dataset.scaled[is.outlier.cerioli,]
rownames(cerioli.outliers) <- is.outlier.cerioli

kable(cerioli.outliers,
      caption = "Valores normalizados para los outliers encontrados con el test de Cerioli de tipo b")

ggplot(bivar.dataset, aes(x=V3, y=V4)) +
    geom_point(colour = "lightblue") +
    geom_point(data=bivar.dataset[is.outlier.cerioli,], colour = "red") +
    geom_point(data=bivar.dataset[180,], colour = "darkgreen")
```

Podemos observar que ahora sí obtenemos resultados más
interesantes. Aunque los valores para las variables del punto con
índice 180, marcado en verde en el gráfico, no son especialmente
anómalos (para la variable V3 el valor está a menos de 1 unidad de la
media, y para la variable V4 a menos de 2), la combinación de valores
sí que es atípica, y por esto el algoritmo nos marca el punto como
anómalo. Algo similar podemos decir de los puntos con índice 163, 202
y 203, aunque para éstos el valor de una de las variables es
ligeramente alto.

Podemos extraer conclusiones interesantes observando los resultados
obtenidos por el test de Rosner y los resultados obtenidos por este
método. El test de Rosner sólo nos arrojó resultados positivos para un
punto en cada una de las dos variables que estamos utilizando.
Probablemente, estos dos puntos correspondan a los dos valores más
extremos en cada uno de los ejes de la nube de puntos anterior (los
que corresponden a los índices 116 y 198 en la tabla). Ahora, al
combinar las dos variables y trabajar con un test multidimensional,
tenemos muchos más registros con valores atípicos, los cuales resultan
más interesantes de estudiar que las anomalías univariadas.

Este test puede ser aplicado a distribuciones de mayor
dimensionalidad, no sólo para distribuciones bidimensionales. Podemos
utilizar el test sobre el conjunto de datos al completo. No obstante,
existe una dependencia lineal muy alta entre algunas de nuestras
variables, y si intentamos ejecutar el algoritmo sobre las 6 columnas
se nos produce un error porque la matriz de covarianzas con la que
trabaja el test se convierte en una matriz singular y no es
diagonalizable. Esto nos ha obligado a descartar la primera variable y
trabajar con las otras cinco. El resultado que obtenemos es el
siguiente (la representación gráfica se realiza sobre las dos primeras
componentes principales del conjunto de datos:


```{r}
multivar.dataset <- dataset[,2:6]
dataset.scaled <- scale(multivar.dataset)
nivel.de.significacion.penalizado = 1 - (1 - nivel.de.significacion) ^ (1 / nrow(multivar.dataset))

## Método de Cerioli para el cálculo de outliers
cerioli <- cerioli2010.fsrmcd.test(
    multivar.dataset, signif.alpha = nivel.de.significacion.penalizado
)

is.outlier.cerioli <- which(cerioli$outliers)
cerioli.outliers <- dataset.scaled[is.outlier.cerioli,]
rownames(cerioli.outliers) <- is.outlier.cerioli

kable(cerioli.outliers,
      caption = "Valores normalizados para los outliers encontrados con el test de Cerioli de tipo b sobre el conjunto de datos completo")

multivar.dataset$outlier <- cerioli$outliers
autoplot(prcomp(multivar.dataset), data = multivar.dataset, colour = 'outlier')
```

Donde de nuevo podemos observar que tenemos algunos outliers
multidimensionales sobre el conjunto completo. Si quitamos
aquellos outliers en los que al menos una variable se desvía
más de $2\sigma$ de su media, los outliers que nos quedan son
los siguientes:

```{r}
extreme.rows <- apply(cerioli.outliers, 1, function(x) any(abs(x) > 2))

kable(cerioli.outliers[!extreme.rows,],
      caption = "Valores outliers multivariados puros")
```

Una vez hemos visto cómo detectar outliers utilizando tests
estadísticos, en el siguiente apartado trataremos de utilizar modelos
basados en distancias para la detección de anomalías.

# Modelos basados en distancias para detección de anomalías

En este apartado, utilizaremos métodos no paramétricos basados en
distancias para la detección de puntos anómalos en nuestro conjunto de
datos. La principal ventaja que tienen estos métodos respecto a los
anteriores es que no necesitan suponer una distribución sobre los
datos a priori. Los test estadísticos que hemos utilizado previamente
asumían que los datos seguían una distribución normal, lo cual es una
asunción bastante fuerte, que no siempre se cumple cuando se trabaja
en un problema real. Los métodos que vamos a utilizar a continuación
no necesitan hacer estas suposiciones, dado que son métodos no
paramétricos.

Vamos a estudiar tres métodos distintos basados en distancias. El
primero de ellos, conocido como Local Outlier Factor, se se basa en la
idea del vecino más cercano. Los dos métodos restantes son
modificaciones de algoritmos clásicos de clustering, concretamente de
k-means y PAM, los cuales marcan los puntos como anómalos en función
de la distancia que haya entre cada punto y el centro del cluster
al que pertenece.

Comenzamos estudiando el método basado en vecindad.

## LOF

En este apartado, veremos un método basado en distancias para la
detección de datos anómalos. Concretamente, este algoritmo, conocido
como Local Outlier Factor (LOF), se basa en la idea de vecinos más
cercanos. Para cada punto del conjunto de datos, se calcula una
puntuación, la cual se calcula como una función dependiente de la
distancia de dicho punto a sus $k$ vecinos más cercanos, cuanto mayor
sea este valor, más aislado del resto de puntos del conjunto se
encuentra el dato en cuestión. Para este algoritmo nos hace falta, por
tanto, establecer la función de distancia con la que trabajaremos, que
en nuestro caso será la distancia euclídea porque todas las variables
con las que trabajamos son numéricas y carecemos de información para
poder utilizar una distancia más apropiada, y el valor de $k$. En
principio, estableceremos dicho valor a 5, ya que no tenemos mucha
información sobre el conjunto de datos.

```{r}
dataset.scaled <- scale(dataset)
numero.de.vecinos.lof = 5

lof.scores <- lofactor(dataset.scaled, numero.de.vecinos.lof)

indices.segun.lof.score.ordenados <- order(lof.scores, decreasing = T)

lof.scores.ordenados <- lof.scores[indices.segun.lof.score.ordenados]
plot(lof.scores.ordenados)
```

En el plot anterior podemos ver como hay un primer grupo de valores
anómalos detectados por este método, y tras un pequeño salto, se
produce una cierta estabilizacion de dichos valores, que empiezan a
decrecer a un ritmo más lento. Concretamente, en el primer grupo hay
siete puntos, contando con el outlier con un valor superior a 6. Este
método no nos marca el número de outliers, si no que nos da un
determinado coeficiente para cada punto. Tras calcular ese
coeficiente, tenemos que decidir nosotros cuál es el valor a partir
del cual se considera que un punto es anómalo. Esta es una de las
problemáticas del algoritmo. El índice de anomalía que se nos da no es
fácilmente interpretable. Por la construcción del mismo, es evidente
que un valor menor o igual que 1 representa un punto no anómalo, pero
para cualquier valor superior a 1, es difícil establecer el punto en
el que los ejemplos empiezan a considerarse anómalos. El valor óptimo
es muy dependiente del dataset, y por lo general difícil de
establecer. Como ya hemos comentado, inspeccionando los valores
ordenados, parece sensato establecer el número de outliers en 7:

```{r}
numero.de.outliers <- 7
indices.de.lof.top.outliers <- indices.segun.lof.score.ordenados[1:numero.de.outliers]

is.lof.outlier <- 1:dim(dataset.scaled)[1] %in% indices.de.lof.top.outliers

MiBiPlot_Multivariate_Outliers(
    dataset.scaled, is.lof.outlier,
    "Outliers detectados por LOF"
)

data.frame.solo.outliers <- dataset.scaled[is.lof.outlier,]

MiBoxPlot_juntos(dataset.scaled, is.lof.outlier)
```

En el boxplot anterior podemos comprobar que muchos de los puntos que
hemos detectado tienen algún outlier univariante. Como lo que nos
interesa con este método es buscar outliers multivariantes, vamos a
tratar de limpiar los elementos anomálos univariantes de las anomalías
que hemos obtenido. Una vez hemos etiquetado los puntos anómalos con
LOF, vamos a eliminar de este conjunto aquellos puntos que tengan
algún outlier univariante. Para esto, extraeremos, utilizando el
método IQR, los registros que tengan algún outlier para alguna
variable.

```{r}
vector.claves.outliers.IQR.en.alguna.columna <- vector_claves_outliers_IQR_en_alguna_columna(dataset.scaled)

indices.de.outliers.multivariantes.LOF.pero.no.1variantes <- setdiff(
    indices.de.lof.top.outliers, vector.claves.outliers.IQR.en.alguna.columna
)

valores.normalizados.de.los.outliers.LOF.pero.no.1variantes <- dataset.scaled[
    indices.de.outliers.multivariantes.LOF.pero.no.1variantes,
    ]

is.lof.multivariate.outlier <- 1:dim(dataset.scaled)[1] %in%
    indices.de.outliers.multivariantes.LOF.pero.no.1variantes

MiBiPlot_Multivariate_Outliers(
    dataset.scaled, is.lof.multivariate.outlier,
    "Outliers multivariantes detectados por LOF"
)

kable(t(valores.normalizados.de.los.outliers.LOF.pero.no.1variantes),
      caption="Valores de las variables normalizadas para el outlier multivariante detectado con LOF")
```

Como podemos observar en la gráfica anterior, casi todos los outliers
que se han detectado por este método son outliers univariantes. Tras
limpiar los outliers detectados por el método IQR, sólo nos ha quedado
un punto relevante. Además, este punto tiene un valor ligeramente alto
para la segunda variable, como se puede ver en la tabla anterior. A
pesar de esto, no es un punto con valores muy extremos, como podemos
comprobar en la nube de puntos previa. El hecho de que aparezca
rodeado de otros puntos nos indica que para ninguna de las variables
se tiene un valor excesivamente alto, y que por tanto es posible
que hayamos encontrado realmente un outlier multivariante.

Una vez hemos visto el funcionamiento del algoritmo basado en vecindad,
pasamos a comentar los métodos basados en clustering.

## Métodos basados en clustering

Los métodos de agrupamiento basados en clustering que vamos a estudiar
se basan todos en la misma idea. Cada cluster del conjunto estará
representado por un elemento característico o centroide, y cada
ejemplo será considerado como más o menos anómalo en función de la
distancia a la que se encuentre del centroide de su grupo. La
diferencia entre los algoritmos que veremos reside en la forma que
tienen de buscar los centroides. Comenzamos con el método conocido
como k-means

### K-means

El algoritmo de k-means o algoritmo de las k-medias es un algoritmo de
agrupamiento cuyo funcionamiento se basa en la construcción de
instancias artificiales, que funcionan como centros de sus
clusters. El funcionamiento del algoritmo es el que sigue. Se debe
establecer a priori el número $k$ de grupos que van a formarse. Una
vez especificado dicho valor:

- Se inicializan aleatoriamente $k$ elementos artificiales en el
  espacio de características (centroides)
- Se asigna cada ejemplo del conjunto al centroide más cercano
- Mientras no se alcance un punto de equilibrio (la asignación de los
  puntos a los centroides no cambie en una iteración)
  - Se recalcula la posición de los centroides como la media
  aritmética de los puntos que pertenecen al cluster
  - Se reasignan los puntos a su cluster más cercano.

Como se puede intuir de la descripción del algoritmo, este método es
dependiente de la función de distancia que se utilice. Usualmente,
para atributos numéricos, suelen usarse distancias de Minkowski, y más
concretamente la distancia de Minkowski para $k=2$ (distancia
euclídea). No obstante, para otro tipo de dato, o con el fin de
aportar información al problema, se puede utilizar otra función de
distancia. No estudiaremos cómo afecta el cambio de la distancia
al resultado del algoritmo, ya que carece de interés por el momento.
Nos limitaremos al uso de la distancia euclídea, la cual está bien
definida en nuestro problema, ya que todos los atributos de nuestro
conjunto de datos son atributos numéricos continuos.

Mostramos una ejecución del algoritmo. Trabajaremos con tres clusters
en primer lugar. En la gráfica puede observarse el resultado de la
ejecución del algoritmo. Es recomendable remarcar que, aunque la
visualización la hemos hecho sobre los datos originales, el algoritmo
se ha ejecutado sobre los datos normalizados, ya que las diferencias
en la varianza de las distintas variables afectan notablemente en las
distancias calculadas, por lo que se debe trabajar con los datos
normalizados para evitar introducir un sesgo.

```{r}
numero.de.clusters <- 3
dataset.scaled <- scale(dataset)
modelo.kmeans <- kmeans(dataset.scaled, numero.de.clusters)
indices.clustering <- modelo.kmeans$cluster

centroides.normalizados <- modelo.kmeans$centers

autoplot(modelo.kmeans, data = dataset)
```

Una vez tenemos los resultados del modelo, se nos proporcionan también
las coordenadas de los tres centroides:

```{r}
kable(centroides.normalizados, row.names=T, caption="Coordenadas normalizadas de los centroides calculados por el algoritmo")
```

Con esta información, podemos calcular las distancias de cada punto a
su centroide. Tras esto, ordenando las distancias obtenidas, tenemos
los puntos que más se alejan de sus respectivos centroides, y por
tanto los datos peor representados por su conjunto. Mostramos a
continuación los primeros outliers de nuestro conjunto de datos,
ordenados en función de la distancia a su centroide:

```{r}
distancias_a_centroides = function (datos.normalizados,
                                    indices.asignacion.clustering,
                                    datos.centroides.normalizados){
    sqrt(rowSums((datos.normalizados - datos.centroides.normalizados[
                                           indices.asignacion.clustering,
                                           ])^2
                 ))
}

dist.centroides <- distancias_a_centroides(dataset.scaled, indices.clustering,
                                           centroides.normalizados)

top.outliers <- order(dist.centroides, decreasing = T)[1:15]

kable(t(dist.centroides[top.outliers]), col.names=top.outliers, digits = 2,
      caption="Índice y distancia de los 15 puntos más alejados de sus respectivos centroides")
```

La problemática principal de este método, al igual que ocurría con
LOF, es el hecho de tener que identificar los outliers a partir de los
valores de distancia anteriores, cosa que puede no ser sencilla en
todos los casos. Observando los valores, tenemos un punto
significativamente alejado de su centroide, tras este otros cuatro
puntos en los que la distancia aún se decrementa rápidamente entre uno
y otro, y después los valores comienzan a estabilizarse. Consideramos,
por tanto, que  existen 5 outliers en nuestro conjunto de datos.
Mostramos dichos outliers en una gráfica sobre el conjunto de datos completo:

```{r}
is.kmeans.outlier <- sapply(1:dim(dataset.scaled)[1], function (x) x %in% top.outliers[1:5])

BIPLOT.isOutlier             = is.kmeans.outlier
BIPLOT.cluster.colors        = c("blue","red","brown")     # Tantos colores como diga numero.de.clusters
BIPLOT.asignaciones.clusters = indices.clustering
MiBiPlot_Clustering_Outliers(dataset, "K-Means Clustering Outliers")
```

En el biplot se muestran los 5 puntos marcados como outliers. Se puede
observar que todos los puntos se encuentran en la parte más externa
del gráfico, lo cual suele indicar que los outliers son univariantes.
En efecto, en la siguiente tabla podemos observar que, en todos los
casos, al menos una de las columnas presenta un valor alto:

```{r}
kmeans.outliers.data <- as.data.frame(dataset.scaled[top.outliers[1:5],])
rownames(kmeans.outliers.data) <- top.outliers[1:5]

kable(kmeans.outliers.data, caption="Valores de las características de los \
primeros 5 outliers encontrados por el algoritmo k-means")
```
